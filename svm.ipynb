{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('/content/train_diabetes_health_indicators.csv', delimiter=',')\n",
    "df.dataframeName = 'diabetes_health.csv'\n",
    "nRow, nCol = df.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valeur manquant\n",
    "print(f'Colonnes avec valeurs manquantes : { df.isnull().sum()} .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les instances dupliquer\n",
    "lignes_dupliquees = df.duplicated()\n",
    "print(df[lignes_dupliquees])\n",
    "\n",
    "#supprision donnees duplique\n",
    "nombre_lignes_dupliquees = df.duplicated().sum()\n",
    "print(f\"Nombre de lignes dupliquées : {nombre_lignes_dupliquees}\")\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lignes_dupliquees = df.duplicated()\n",
    "print(df[lignes_dupliquees])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Les valeurs que prend la cible sont {df[\"Diabetes_012\"].unique()} ')\n",
    "diabetes_target = df[\"Diabetes_012\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = diabetes_target.value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90, colors=['#66b3ff', '#99ff99', '#ffcc99'])\n",
    "plt.title('Proportion des Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_catégorielles = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "#print(\"Colonnes catégorielles :\")\n",
    "print(colonnes_catégorielles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "df_grouped = df.groupby([\"Diabetes_012\"]).mean().apply(lambda col: zscore(col))\n",
    "\n",
    "df_grouped.T.plot.bar()\n",
    "plt.show()\n",
    "\n",
    "df_0 = df[df[\"Diabetes_012\"] == 0]\n",
    "df_1 = df[df[\"Diabetes_012\"] == 1]\n",
    "df_2 = df[df[\"Diabetes_012\"] == 2]\n",
    "\n",
    "X_0 = df_0.drop(\"Diabetes_012\", axis=1)\n",
    "X_1 = df_1.drop(\"Diabetes_012\", axis=1)\n",
    "X_2 = df_2.drop(\"Diabetes_012\", axis=1)\n",
    "\n",
    "X_0_normalized = X_0.apply(lambda col: zscore(col))\n",
    "X_1_normalized = X_1.apply(lambda col: zscore(col))\n",
    "X_2_normalized = X_2.apply(lambda col: zscore(col))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns[1:]:  # Excluez la colonne 'Target'\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.boxplot([X_0[col], X_1[col]])\n",
    "    plt.title(f'Histogramme de {col} pour les classes 0 et 1')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".1f\", linewidths=0.5)\n",
    "plt.title('Matrice de Corrélation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour verifier si on peut supprimer les variable qui ont correlation plus que 0,8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supposons que vous ayez déjà défini df\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "correlation_matrix = df.corr().abs()\n",
    "\n",
    "# Sélectionner la partie supérieure de la matrice de corrélation\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(np.bool_))\n",
    "\n",
    "# Trouver les colonnes corrélées avec un seuil\n",
    "threshold = 0.8\n",
    "correlated_columns = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "\n",
    "# Supprimer les colonnes corrélées\n",
    "df_no_corr = df.drop(columns=correlated_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "# Apply standardization to the entire DataFrame\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Diabetes_012\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_standardized, diabetes_target, stratify=diabetes_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez des poids d'échantillonnage en fonction des classes\n",
    "class_weights = {0: 1, 1: 2, 2: 1}  # Ajustez les poids selon vos besoins\n",
    "\n",
    "# Créez un tableau de poids pour chaque exemple d'entraînement\n",
    "sample_weights = [class_weights[label] for label in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilisation de cross validation\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# Utilisez la validation croisée stratifiée avec 5 plis\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Définissez les hyperparamètres que vous souhaitez optimiser\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],          # Paramètre de régularisation\n",
    "    'gamma': [0.01, 0.1, 1, 'auto'], # Paramètre du noyau (pour 'rbf')\n",
    "    'kernel': ['linear', 'rbf']     # Type de noyau\n",
    "}\n",
    "\n",
    "# Créez un modèle SVM\n",
    "svm_model = SVC(class_weight=class_weights, random_state=42)\n",
    "\n",
    "# Utilisez GridSearchCV pour trouver les meilleurs hyperparamètres\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, scoring='roc_auc', cv=stratified_kfold, n_jobs=-1)\n",
    "\n",
    "# Ajustez le modèle aux données d'entraînement\n",
    "grid_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Obtenez les meilleurs hyperparamètres trouvés\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Obtenez le meilleur modèle entraîné avec les meilleurs hyperparamètres\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Prédisez les classes sur les données de test avec le meilleur modèle\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Mesurez les performances du meilleur modèle\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, best_model.decision_function(X_test), multi_class='ovo')\n",
    "\n",
    "print(f'Meilleurs hyperparamètres : {best_params}')\n",
    "print(f'Accuracy du meilleur modèle : {accuracy:.4f}')\n",
    "print(f'ROC AUC du meilleur modèle : {roc_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model, open(\"SVM.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
